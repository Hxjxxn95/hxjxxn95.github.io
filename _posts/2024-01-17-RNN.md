---
layout : post
title: RNN
author: Hojoon_Kim
date: 2024-01-17 15:15:10 +0900
categories: [Develope, DL]
tags: [Pytorch, DL, RNN, LSTM, GRU]
pin: true
math: true
mermaid: true
---
## 순환 신경망 (Recurrent Neural Network, RNN)
- 순환 신경망은 시퀀스(Sequence) 모델이다. 시퀀스는 음악, 문장, 동영상 등의 데이터를 말한다. 이러한 시퀀스 데이터는 시간의 흐름에 따라 데이터가 순차적으로 등장한다는 특징을 가진다. 이러한 특징을 가진 데이터를 다루기 위해 등장한 것이 순환 신경망이다.
- 순환 신경망은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징을 가지고 있다. 이는 은닉층의 노드에서 이전의 계산 결과를 기억하려는 특징이 있다. 이를 메모리라고 한다.
- RNN 은 입력과 출력의 길이를 다르게 설계할 수 있다. 입력과 출력의 길이가 같은 경우는 many-to-many, 입력의 길이가 더 긴 경우는 many-to-one, 출력의 길이가 더 긴 경우는 one-to-many, 입력과 출력의 길이가 1인 경우는 one-to-one이다.
![image](https://wikidocs.net/images/page/22886/rnn_image4_ver2.PNG)    

-> 은닉층 : $ h_t = tanh(W_{xh}x_t + W_{hh}h_{t-1} + b_h) $

-> 출력층 : $ y_t = f(W_{hy}h_t + b_y) $ 

### 파이썬으로 RNN 구현
```python
import numpy as np

timestep = 10 # 시점의 수. NLP에서는 보통 문장의 길이가 된다.
input_size = 4 # 입력의 차원. NLP에서는 보통 단어 벡터의 차원이 된다.
hidden_size = 8 # 은닉 상태의 크기. 메모리 셀의 용량이다.

inputs = np.random.random((timestep, input_size)) # 입력에 해당되는 2D 텐서
hidden_state_t = np.zeros((hidden_size,)) # 초기 은닉 상태는 0(벡터)로 초기화
Wx = np.random.random((hidden_size, input_size)) # (8, 4)크기의 2D 텐서 생성. 입력에 대한 가중치.
Wh = np.random.random((hidden_size, hidden_size)) # (8, 8)크기의 2D 텐서 생성. 은닉 상태에 대한 가중치.
b = np.random.random((hidden_size,)) # (8,)크기의 1D 텐서 생성. 이 값은 편향(bias).

total_hidden_states = []

# 메모리 셀 동작
for input_t in inputs: # 각 시점에 따라서 입력값이 입력됨.
    output_t = np.tanh(np.dot(Wx, input_t) + np.dot(Wh, hidden_state_t) + b) # Wx * Xt + Wh * Ht-1 + b(bias)
    total_hidden_states.append(list(output_t)) # 각 시점의 은닉 상태의 값을 계속해서 축적
    print(np.shape(total_hidden_states)) # 각 시점 t별 메모리 셀의 출력의 크기는 (timestep, output_dim)

    hidden_state_t = output_t

total_hidden_states = np.stack(total_hidden_states, axis = 0)
print(total_hidden_states) # (timestep, output_dim)의 크기. 이 경우 (10, 8)의 크기를 가지는 메모리 셀의 2D 텐서를 출력
```

## 장단기 메모리( Long Short-Term Memory, LSTM)
- RNN은 시점이 길어질수록 앞의 정보가 뒤로 충분히 전달되지 못하는 단점이 있다. 이를 장기 의존성 문제(The problem of Long-Term Dependencies)라고 한다. 이를 보완하기 위해 고안된 것이 LSTM이다.

![image](https://wikidocs.net/images/page/22888/vaniila_rnn_and_different_lstm_ver2.PNG)

### 입력 게이트
![image](https://wikidocs.net/images/page/22888/inputgate.PNG)  

입력 게이트는 현재 정보를 기억하기 위한 게이트이다. $ i_t $ 는 현재 시점 t의 입력값이다. $ W_{xi} $ 는 입력에 대한 가중치이다. $ W_{hi} $ 는 이전 시점 t-1의 은닉 상태에 대한 가중치이다. $ b_i $ 는 입력에 대한 편향이다. 시그모이드 함수를 지난 값은 0과 1사이의 값이 되며, 이 값이 곧 게이트의 열림 상태를 결정한다. 0에 가까울수록 닫힌 상태, 1에 가까울수록 열린 상태를 의미한다. $ tanh $ 를 지난 값은 -1과 1사이의 값이 되며, 이 값은 현재 시점의 후보 값이다. 이 두 개의 값을 가지고 이번에 선택된 값을 계산한다. 이를 현재 시점의 셀 상태라고 한다.

$$ i_t = \sigma(W_{xi}x_t + W_{hi}h_{t-1} + b_i) $$
$$ \tilde{C}_t = tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c) $$

### 삭제 게이트
![image](https://wikidocs.net/images/page/22888/forgetgate.PNG) 

삭제 게이트는 기억을 삭제하기 위한 게이트이다. $ f_t $ 는 이전 시점 t-1의 셀 상태값이다. 시그모이드 함수를 지난 값은 0과 1사이의 값이 되며, 이 값이 곧 게이트의 열림 상태를 결정한다. 0에 가까울수록 닫힌 상태, 1에 가까울수록 열린 상태를 의미한다. 이 값을 현재 시점의 셀 상태값과 곱해주면 이전 시점의 셀 상태값을 얼마나 반영할지 결정한다.

$$ f_t = \sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_f) $$

### 셀 상태(장기 상태)

![image](https://wikidocs.net/images/page/22888/cellstate2.PNG) 

이전 시점의 셀 상태값에 현재 시점의 입력값을 반영한 값이다. 이 값은 두 개의 게이트에 의해서만 결정된다. 이전 시점의 셀 상태값인 $ C_{t-1} $ 에서 삭제 게이트를 지난 값이 곱해지고, 입력 게이트를 지난 $ \tilde{C}_t $ 가 더해져서 현재 시점의 셀 상태값이 된다. 결과적으로 삭제게이트는 이전 시점의 입력을 얼마나 반영할지를 결정하고, 입력게이트는 현재 시점의 입력을 얼마나 반영할지를 결정한다.

$$ C_t = f_t * C_{t-1} + i_t * \tilde{C}_t $$

### 출력 게이트와 은닉 상태(단기 상태)

![image](https://wikidocs.net/images/page/22888/outputgateandhiddenstate.PNG)

출력 게이트는 현재 시점 t의 입력값과 이전 시점 t-1의 은닉 상태값을 가지고 시그모이드 함수를 통해 값을 결정한다. 이 값은 현재 시점 t의 셀 상태값을 $ tanh $ 함수를 통해 값의 범위를 -1과 1사이의 값으로 변환한 후, 출력 게이트의 값과 곱해서 현재 시점 t의 은닉 상태값을 계산한다. 이 값은 다음 시점 t+1의 LSTM 셀로 넘겨진다.

$$ o_t = \sigma(W_{xo}x_t + W_{ho}h_{t-1} + b_o) $$

## 게이트 순환 유닛 (Gated Recurrent Unit, GRU)
LSTM 에서는 출력, 입력, 삭제 게이트가 존재하지만, GRU 에서는 업데이트 게이트와 리셋 게이트라는 두 가지 게이트가 존재한다. GRU 는 LSTM 보다 학습 속도가 빠르다는 장점이 있다.

![image](https://wikidocs.net/images/page/22889/gru.PNG)

### 업데이트 게이트

업데이트 게이트는 LSTM 의 입력 게이트와 삭제 게이트와 유사하다. 시그모이드 함수를 지난 값이 곧 게이트의 열림 상태를 결정한다. 0에 가까울수록 닫힌 상태, 1에 가까울수록 열린 상태를 의미한다. 이 값이 곧 이번에 선택된 기억할 정보의 양이다. 이 값을 $ z_t $ 라고 하자.

$$ z_t = \sigma(W_{xz}x_t + W_{hz}h_{t-1} + b_z) $$

### 리셋 게이트

리셋 게이트는 과거의 정보를 잊기 위한 게이트이다. 시그모이드 함수를 지난 값이 곧 게이트의 열림 상태를 결정한다. 0에 가까울수록 닫힌 상태, 1에 가까울수록 열린 상태를 의미한다. 이 값을 $ r_t $ 라고 하자.

$$ r_t = \sigma(W_{xr}x_t + W_{hr}h_{t-1} + b_r) $$

### 은닉 상태(단기 상태)

은닉 상태를 $ h_t $ 라고 하자. 은닉 상태의 크기는 LSTM 의 은닉 상태의 크기와 동일하다. 은닉 상태를 계산하기 위해서 먼저 이전 시점의 은닉 상태를 리셋 게이트의 값으로 곱해준다. 이것이 곧 이전 시점의 은닉 상태에서 얼마나 많은 과거의 정보를 잊어버릴지를 정하는 것이다. 그리고 이전 시점의 은닉 상태를 리셋 게이트의 값으로 곱한 결과와 현재 시점의 입력값을 업데이트 게이트의 값으로 곱한 결과를 더한다. 이것이 곧 현재 시점의 은닉 상태가 된다.

$$ h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t $$



## REFERENCE
[위키독스](https://wikidocs.net/60760)