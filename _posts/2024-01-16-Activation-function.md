---
layout : post
title: Activation function
author: Hojoon_Kim
date: 2024-01-16 19:15:10 +0900
categories: [Develope, DL]
tags: [Pytorch, DL, Activation function]
pin: true
math: true
mermaid: true
---

## 비선형 활성화 함수 (Nonlinear Activation Function)
비선ㅁ형 활성화 함수는 입력을 받아서 비선형 변환을 수행한다. 비선형 활성화 함수를 사용하지 않으면 인공 신경망은 선형 함수의 조합이 되므로, 은닉층을 여러 개로 구성하는 의미가 없어진다. 선형 함수의 조합은 결국 선형 함수이기 때문이다. 

1. 시그모이드 함수 (Sigmoid Function)   
시그모이드 함수는 출력값이 0 또는 1에 가까워지면 기울기의 값이 0에 가까워진다. 이는 은닉층이 깊어지면 이전 층의 기울기를 전파받지 못하게 되면서 기울기 소실 문제가 발생한다. 또한 항상 양수를 출력하기때문에 편향 이동 문제가 발생한다.

2. 하이퍼볼릭탄젠트 함수 (Hyperbolic tangent     function)
하이퍼볼릭탄젠트 함수는 시그모이드 함수와 비슷하지만, -1과 1사이의 값을 가진다는 차이점이 있다. 하이퍼볼릭탄젠트 함수도 시그모이드 함수와 마찬가지로 출력값이 0 또는 1에 가까워지면 기울기가 0에 가까워진다. 이는 은닉층이 깊어지면 이전 층의 기울기를 전파받지 못하게 되면서 기울기 소실 문제가 발생한다.

3. 렐루 함수 (ReLU)  
렐루 함수는 입력이 0을 넘으면 입력을 그대로 출력하고, 0 이하이면 0을 출력한다. 렐루 함수는 시그모이드 함수나 하이퍼볼릭탄젠트 함수와 달리 입력값이 0이 되면 기울기가 0이 되는 구간이 존재하지 않는다. 이는 은닉층이 깊어져도 기울기 소실 문제가 발생하지 않는다는 장점이 있다. 하지만, 입력값이 음수면 기울기가 0이 되는 문제(죽은 렐루)가 있다.

4. 리키 렐루 함수 (Leaky ReLU)  
리키 렐루 함수는 입력값이 음수일 때, 0이 아닌 매우 작은 수를 출력한다. 이는 입력값이 음수일 때, 기울기가 0이 되는 문제를 해결한다.

### 추천 활성화 함수
- 은닉층에서는 ReLU 함수를 사용한다.
- 이진 분류를 수행하는 출력층에서는 시그모이드 함수를 사용한다.
- 다중 클래스 분류를 수행하는 출력층에서는 소프트맥스 함수를 사용한다.

- 스탠포드 대학교의 딥 러닝 강의 cs231n에서는 ReLU를 먼저 시도해보고, 그다음으로 LeakyReLU나 ELU 같은 ReLU의 변형들을 시도해보며, sigmoid는 사용하지 말라고 권장합니다.


## 기울기 소실 (Gradient Vanishing) 문제
기울기 소실을 막기 위해 시그모이드 함수 대신 렐루 같은 함수를 사용한다. 그리고 그 밖에도 가중치 초기화, 배치 정규화, 그래디언트 클리핑 등의 방법들이 사용된다.

### 가중치 초기화 (Weight initialization)
1. 세이비어 초기화 (Xavier Initialization)
- 정규 분포로 초기화 할 때 두 가지 경우로 나뉘며, 이전 층의 뉴런 개수와 다음 층의 뉴런 개수를 가지고 식을 세운다. 이전 층의 뉴런 개수를 fan-in, 다음 층의 뉴런 개수를 fan-out이라고 하였을 때, 두 경우에 대한 식은 아래와 같다.

$$ W \sim Uniform(-\sqrt{\frac{6}{fan_{in}+fan_{out}}}, \sqrt{\frac{6}{fan_{in}+fan_{out}}}) $$

- 평균은 0, 표준편차는 $ \sqrt{\frac{2}{fan_{in}+fan_{out}}} $인 정규 분포를 가지는 초기값을 사용한다.
- 세이비어 초기화는 각 층의 입력과 출력 개수를 반영하여 초기화한다. 세이비어 초기화는 시그모이드 함수와 하이퍼볼릭탄젠트 함수를 사용하는 경우에 적합하다.
- 렐루와 같은 함수에서는 좋지는 않음.

2. He 초기화 (He Initialization)
- 정규 분포로 초기화 할 때 두 가지 경우로 나뉘며, 이전 층의 뉴런 개수와 다음 층의 뉴런 개수를 가지고 식을 세운다. 이전 층의 뉴런 개수를 fan-in, 다음 층의 뉴런 개수를 fan-out이라고 하였을 때, 두 경우에 대한 식은 아래와 같다.

$$ W \sim Uniform(-\sqrt{\frac{6}{fan_{in}}}, \sqrt{\frac{6}{fan_{in}}}) $$

- 평균은 0, 표준편차는 $ \sqrt{\frac{2}{fan_{in}}} $인 정규 분포를 가지는 초기값을 사용한다.

- He 초기화는 ReLU 계열 함수를 사용하는 경우에 적합하다.

### 배치 정규화 (Batch Normalization)
- 배치 정규화는 들어오는 배치 단위로 정규화 하는 것을 의미한다.
- 요약하면, 입력 평균을 0으로 만들고, 입력 분산을 1로 만들어서, 그리고 이에 대한 스케일과 시프트를 수행하는 것이다.
- 배치 정규화를 사용하면 학습 속도가 개선되고, 초기값에 크게 의존하지 않아도 된다.
- 배치 정규환는 학습 시의 배치 단위의 평균과 분산을 사용하지만, 테스트 시에는 전체 데이터의 평균과 분산을 사용한다. 이는 테스트 데이터가 적을 때, 학습 시의 데이터의 평균과 분산에 의존하는 것을 방지하기 위함이다.

- 배치 정규화의 효과는 굉장하지만, 단점도 있다. 배치 정규화를 사용하면 미니 배치 크기에 의존적이 된다는 것이다. 배치 크기를 작게 가져가면 잘 동작하지 않을 수 있다. 또한 배치 정규화의 연산은 미니 배치마다 평균과 분산을 계산하고, 정규화를 수행하고, 이에 대한 스케일과 시프트를 수행하는 연산이 추가적으로 필요하기 때문에, 학습 시간이 늘어난다는 단점이 있다.
- RNN 계열에서는 배치 정규화를 사용하지 않는다. 이는 RNN이 각 시점(time step)마다 다른 통계치를 가지기 때문이다.

### 층 정규화 (Layer Normalization)
- 배치 정규화는 배치 차원에 대한 정규화를 수행한다. 층 정규화는 배치 차원 대신에 특성 차원에 대한 정규화를 수행한다. 층 정규화는 RNN에도 적용할 수 있다.
- 배치 정규화 보다는 계산이 많고, 시계열 데이터에는 잘 동작하지 않는다는 단점이 있다.