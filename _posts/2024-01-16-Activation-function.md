---
layout : post
title: Activation function
author: Hojoon_Kim
date: 2024-01-16 19:15:10 +0900
categories: [Develope, DL]
tags: [Pytorch, DL, Activation function]
pin: true
math: true
mermaid: true
---

## 비선형 활성화 함수 (Nonlinear Activation Function)
비선ㅁ형 활성화 함수는 입력을 받아서 비선형 변환을 수행한다. 비선형 활성화 함수를 사용하지 않으면 인공 신경망은 선형 함수의 조합이 되므로, 은닉층을 여러 개로 구성하는 의미가 없어진다. 선형 함수의 조합은 결국 선형 함수이기 때문이다. 

1. 시그모이드 함수 (Sigmoid Function)   
시그모이드 함수는 출력값이 0 또는 1에 가까워지면 기울기의 값이 0에 가까워진다. 이는 은닉층이 깊어지면 이전 층의 기울기를 전파받지 못하게 되면서 기울기 소실 문제가 발생한다. 또한 항상 양수를 출력하기때문에 편향 이동 문제가 발생한다.

2. 하이퍼볼릭탄젠트 함수 (Hyperbolic tangent     function)
하이퍼볼릭탄젠트 함수는 시그모이드 함수와 비슷하지만, -1과 1사이의 값을 가진다는 차이점이 있다. 하이퍼볼릭탄젠트 함수도 시그모이드 함수와 마찬가지로 출력값이 0 또는 1에 가까워지면 기울기가 0에 가까워진다. 이는 은닉층이 깊어지면 이전 층의 기울기를 전파받지 못하게 되면서 기울기 소실 문제가 발생한다.

3. 렐루 함수 (ReLU)  
렐루 함수는 입력이 0을 넘으면 입력을 그대로 출력하고, 0 이하이면 0을 출력한다. 렐루 함수는 시그모이드 함수나 하이퍼볼릭탄젠트 함수와 달리 입력값이 0이 되면 기울기가 0이 되는 구간이 존재하지 않는다. 이는 은닉층이 깊어져도 기울기 소실 문제가 발생하지 않는다는 장점이 있다. 하지만, 입력값이 음수면 기울기가 0이 되는 문제(죽은 렐루)가 있다.

4. 리키 렐루 함수 (Leaky ReLU)  
리키 렐루 함수는 입력값이 음수일 때, 0이 아닌 매우 작은 수를 출력한다. 이는 입력값이 음수일 때, 기울기가 0이 되는 문제를 해결한다.

### 추천 활성화 함수
- 은닉층에서는 ReLU 함수를 사용한다.
- 이진 분류를 수행하는 출력층에서는 시그모이드 함수를 사용한다.
- 다중 클래스 분류를 수행하는 출력층에서는 소프트맥스 함수를 사용한다.

- 스탠포드 대학교의 딥 러닝 강의 cs231n에서는 ReLU를 먼저 시도해보고, 그다음으로 LeakyReLU나 ELU 같은 ReLU의 변형들을 시도해보며, sigmoid는 사용하지 말라고 권장합니다.



