---
layout : post
title: TF-IDF
author: Hojoon_Kim
date: 2024-01-22 15:15:10 +0900
categories: [Develope, NLP]
tags: [Pytorch, DL, NLP, TF-IDF]
pin: true
math: true
mermaid: true
---
TF-IDF 는 단어의 중요도를 계산하는 방법이다. TF-IDF 는 단어의 빈도와 역 문서 빈도를 사용하여 계산한다. TF-IDF 는 단어의 빈도가 높고, 문서의 단어가 적을수록 TF-IDF 값이 높아진다. TF-IDF 는 문서의 핵심어를 추출하는데 사용된다.

## TF-IDF 의 계산
### TF(d,t)
TF 는 단어의 빈도를 의미한다. TF 는 문서 d 에서 단어 t 가 나타나는 횟수이다. TF 는 문서마다 다르게 계산된다. 예를 들어, 문서 d1 에서 단어 t 가 3번 나타나고, 문서 d2 에서 단어 t 가 5번 나타나면, TF(d1,t) = 3, TF(d2,t) = 5 이다.
### DF(t)
DF 는 단어의 역 문서 빈도를 의미한다. DF 는 단어 t 가 나타난 문서의 수이다. 예를 들어, 단어 t 가 문서 d1, d2, d3 에서 나타나면, DF(t) = 3 이다.
### IDF(t)
IDF 는 단어의 역 문서 빈도의 역수를 의미한다. IDF 는 다음과 같이 계산된다.
$$IDF(t) = log(\frac{N}{1+DF(t)})$$

## 코사인 유사도
코사인 유사도는 두 벡터의 유사도를 계산하는 방법이다. 코사인 유사도는 두 벡터의 방향이 얼마나 유사한지를 계산한다. 코사인 유사도는 다음과 같이 계산된다.
$$cos(\theta) = \frac{A \cdot B}{||A|| \cdot ||B||}$$

이런 코사인 유사도로 문서의 유사도를 계산할 수 있다. 문서의 유사도는 문서의 TF-IDF 벡터의 코사인 유사도로 계산한다.

## TF-IDF 예제
```
문서 1: 저는 사과 좋아요
문서 2: 저는 바나나 좋아요
문서 3: 저는 바나나 좋아요 저는 바나나 좋아요
```
||사과|바나나|저는|좋아요|
|---|---|---|---|---|
|문서1|1|0|1|1|
|문서2|0|1|1|1|
|문서3|0|2|2|2|

```python
import numpy as np
from numpy import dot
from numpy.linalg import norm

def cos_sim(A, B):
  return dot(A, B)/(norm(A)*norm(B))

doc1 = np.array([0,1,1,1])
doc2 = np.array([1,0,1,1])
doc3 = np.array([2,0,2,2])

print('문서 1과 문서2의 유사도 :',cos_sim(doc1, doc2))
print('문서 1과 문서3의 유사도 :',cos_sim(doc1, doc3))
print('문서 2와 문서3의 유사도 :',cos_sim(doc2, doc3))

```
```
문서 1과 문서2의 유사도 : 0.67
문서 1과 문서3의 유사도 : 0.67
문서 2과 문서3의 유사도 : 1.00
```
