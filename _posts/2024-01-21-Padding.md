---
layout : post
title: Padding
author: Hojoon_Kim
date: 2024-01-21 15:30:10 +0900
categories: [Develope, NLP]
tags: [Pytorch, DL, NLTK, Padding]
pin: true
math: true
mermaid: true
---
자연어를 처리 할 때 병렬 연산을 위해서는 각 문자 길이를 동일하게 해주는 작업이 필요하다. 이를 위해 Padding을 사용한다. Padding은 가장 긴 문자열을 기준으로 나머지 문자열의 길이를 맞춰주는 작업이다. Padding을 사용하면 가장 긴 문자열보다 짧은 문자열은 0으로 채워주게 된다. 이를 Zero Padding이라고 한다. 하지만 예외적으로 긴 문장이 있다고 해서 모든 문장을 길이를 똑같이 하는 것은 비효율적이다. 그래서 Padding을 할 때는 적절한 길이를 설정해주어야 한다. 이를 위해 가장 긴 문자열의 길이를 구해야 한다. 이를 위해 다음과 같은 코드를 사용한다.

## Keras Padding
```python
from tensorflow.keras.preprocessing.sequence import pad_sequences

encoded = tokenizer.texts_to_sequences(preprocessed_sentences)

# [[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]

padded = pad_sequences(encoded, padding = 'post', maxlen = 5)
# 'post'는 뒤에 패딩을 채우겠다는 의미이다. 'pre'는 앞에 패딩을 채우겠다는 의미이다.
# maxlen은 최대 길이를 설정한다. 이보다 긴 문장은 잘라낸다.

```
```
array([[ 1,  5,  0,  0,  0],
       [ 1,  8,  5,  0,  0],
       [ 1,  3,  5,  0,  0],
       [ 9,  2,  0,  0,  0],
       [ 2,  4,  3,  2,  0],
       [ 3,  2,  0,  0,  0],
       [ 1,  4,  6,  0,  0],
       [ 1,  4,  6,  0,  0],
       [ 1,  4,  2,  0,  0],
       [ 3,  2, 10,  1, 11],
       [ 1, 12,  3, 13,  0]], dtype=int32)
```


