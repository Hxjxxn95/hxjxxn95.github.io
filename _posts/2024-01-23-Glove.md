---
layout : post
title: Glove
author: Hojoon_Kim
date: 2024-01-23 15:15:10 +0900
categories: [Develope, NLP]
tags: [DL, NLP, Glove]
pin: true
math: true
---
TF-IDF는 전체적인 통계 정보로 중요한 의미를 이끌어내는 방법론이였고, Word2Vec은 단어의 의미를 벡터화 하여 단어를 예측하는 방법이였다. 하지만 이런 것들은 각각 장단점이 있다. 이런 것들의 장점을 모아서 만든 것이 Glove 이다. Glove는 LSA의 카운트 기반의 방법과, Word2Vec 의 예측 기반의 방법을 모두 사용한다.

## 윈도우 기반 동시 등장 행렬(Window based Co-occurrence Matrix)
동시 등장 행렬은 단어의 동시 등장 횟수를 집계한 행렬이다. 예를 들어서 다음과 같은 문장이 있다고 하자.
```
I like deep learning
I like NLP
I enjoy flying
```
윈도우 크기가 N 일 때는 좌,우에 존재하는 N개의 단어만참고하게 된다. 예를 들어서 윈도우 크기가 1일 때, "I"의 동시 등장 횟수는 "like"와 "enjoy"에서 각각 2회, 1회 이다. 이런 식으로 동시 등장 행렬을 만들 수 있다.
|카운트|I|like|enjoy|deep|learning|NLP|flying|
|---|---|---|---|---|---|---|---|
|I|0|2|1|0|0|0|0|
|like|2|0|0|1|0|1|0|
|enjoy|1|0|0|0|0|0|1|
|deep|0|1|0|0|1|0|0|
|learning|0|0|0|1|0|0|0|
|NLP|0|1|0|0|0|0|0|
|flying|0|0|1|0|0|0|0|

## 동시 등장 확률(Co-occurrence Probability)
동시 등장 확률은 두 단어의 동시 등장 횟수를 전체 동시 등장 횟수로 나눈 것이다. 예를 들어 특정 단어 i 가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률이다. 이를 수식으로 나타내면 다음과 같다.


$P(k\ |\ i)$ 에서 i를 중심단어(center word), k를 주변 단어(context word)라고 한다. 동시 등장 행렬에서 중심단어 i의 행의 모든 값을 더한 값을 분모로 하고, i 행 k열의 값을 분자로 한 값이다.

|동시 등장 확률과 크기 관계 비|k = Solid|k = Gas|k = Water|k = Fashion|
|---|---|---|---|---|
| P(k/ice) |0.00019|0.000066|0.003|0.000017|
| P(k/steam) |0.000022|0.00078|0.0022|0.000018|
| P(k/ice)/P(k/steam) |8.9|0.085|1.36|0.96|

위의 표에서 알 수 있는 것은 ice 가 가지는 의미가 solid 와 가깝고, steam 이 가지는 의미가 gas 와 가깝다는 것을 알 수 있다. 또한 k 가 water 인 경우에는 solid 와 steam 두 단어 모두와 동시에 등장하는 경우가 많으므로 1에 가까운 값이 나오게 된다. 이런 식으로 동시 등장 확률을 계산하면 단어 간 유사도를 계산할 수 있다.

## 손실 함수(loss function)
- $ X $ : 동시 등장 행렬
- $ X_{ij} $ : 중심 단어 i 와 주변 단어 j 가 동시에 등장한 횟수
- $ X_i = \sum_{j}X_{ij} $ : 중심 단어 i 와 동시에 등장한 모든 단어의 수
- $ P_{ij} = P(j\ |\ i) = \frac{X_{ij}}{X_i} $ : 중심 단어 i 가 등장했을 때, 주변 단어 j 가 등장할 확률
- $ w_i $ : 중심 단어 i 의 임베딩 벡터
- $ \tilde{w_j} $ : 주변 단어 j 의 임베딩 벡터

Glove 는 중심 단어와 주변 단어의 내적이 동시 등장 확률의 로그값과 가까워지도록 학습한다. 이를 수식으로 나타내면 다음과 같다.

$$ dot product(w_i, \tilde{w_j}) = log(P_{ij}) $$

또한 손실 함수는 다음과 같다.

$$ Loss function = \sum_{m,n=1}^{V}f(X_{mn})(w_m^T \tilde{w_n} + b_m + \tilde{b_n} - logX_{mn})^2 $$
$$ f(x) = min(1, (x/x_{min})^{3/4}) $$

여기서 $ V $ 는 단어 집합의 크기이다. $ b_m $ 과 $ \tilde{b_n} $ 은 편향값이다. 이를 최소화하는 방향으로 학습한다.

## Reference
[위키독스 - 딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/book/2155)