---
layout : post
title: Word Embedding
author: Hojoon_Kim
date: 2024-01-22 16:15:10 +0900
categories: [Develope, NLP]
tags: [DL, NLP, Word-Embedding]
pin: true
math: true
---
워드 임베딩이랑 단어를 벡터로 표현한믄 방법으로, 단어를 밀집 표현으로 변환하는 방법이다. 원-핫 인코딩은 단어를 표현하기 위해 벡터의 차원이 단어 집합의 크기만큼 필요했다. 원-핫 벡터는 단어의 유사도를 표현하지 못한다는 단점이 있다. 이를 위해 단어를 밀집 벡터로 표현하는 방법이 있다. 밀집 벡터는 원-핫 벡터와 달리 대부분의 값이 실수이다. 밀집 벡터는 단어의 의미를 잘 표현해준다. 이를 위해 사용하는 방법이 워드 임베딩이다. 워드 임베딩은 단어를 밀집 벡터로 만드는 것을 말한다. 워드 임베딩 방법으로는 LSA, Word2Vec, FastText, Glove 등이 있다.

## 밀집 표현 (Dense Representation)
희소 표현과 반대되는 표현으로, 벡터의 차원을 단어 집합의 크기로 상정하지 않는다. 그래서 대부분의 값이 실수이다. 이를 통해 단어의 의미를 잘 표현해준다.  
ex) [0.1, 0.2, 0.5, -0.9, 1.8]

## 워드투벡터(Word2Vec)
워드투벡터는 단어의 의미를 수치화 할 수 있는 방법이다. 예를 들자면  
한국 - 서울 + 도쿄 = 일본   
박찬호 - 야구 + 축구 = 호나우두 

와 같이 단어의 의미를 벡터화 할 수 있다.

### CBOW(Continuous Bag of Words)
CBOW는 주변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측하는 방법이다. 예를 들어서 "I live in Korea" 라는 문장이 있을 때, 중간에 있는 단어를 예측하는 것이다. 이때 중간에 있는 단어를 예측하기 위해 앞 뒤로 몇 개의 단어를 볼지 결정한다. 이를 윈도우라고 한다. 윈도우 크기가 2라고 한다면, 중간에 있는 단어를 예측하기 위해 앞 뒤로 2개의 단어를 참고한다. 이때 참고하는 단어의 개수를 윈도우 크기라고 한다. 윈도우 크기가 2라면, 윈도우는 총 5개의 단어로 구성된다. 이때 윈도우를 슬라이딩 해가면서 학습을 위한 데이터를 만든다. 
![CBOW](https://wikidocs.net/images/page/22660/word2vec_renew_2.PNG)    
입력에는 주변 단어가 들어가게 되고, 출력층에는 예측하고자 하는 중간 단어가 원-핫 벡터 레이블로 들어가게 된다.
![CBOW](https://wikidocs.net/images/page/22660/word2vec_renew_4.PNG)    
주변 단어의 원-핫 벡터들이 가중치 W가 곱해져서 생긴 결과들이 투사층에서 만나서 평균을 구하게 되고, 이 값들은 가중치 행렬 W'와 곱해져서 각 중간 단어들의 원-핫 벡터와의 오차를 구하게 된다. 이제 이 오차를 손실 함수로 backpropagation을 수행하면서 학습하게 된다. 이렇게 학습이 다 되면 W 행렬의 값들이 워드 임베딩 결과로 나오게 된다.

### Skip-gram
Skip-gram은 중간에 있는 단어로 주변 단어들을 예측하는 방법이다. CBOW와는 반대로 중간에 있는 단어가 입력이 되고, 주변 단어들이 출력이 된다. CBOW와 마찬가지로 윈도우라는 크기를 정하고, 윈도우 안에 있는 단어들을 예측하게 된다.전반적으로 CBOW보다 성능이 좋다고 알려져 있다.
![Skip-gram](https://wikidocs.net/images/page/22660/skipgram_dataset.PNG)

### 패스트 텍스트(FastText)
패스트 텍스트는 페이스북에서 개발안 Word2Vec의 확장이다. Word2Vec은 단어를 쪼개질 수 없는 단위로 생각한다. 하지만 패스트 텍스트는 단어를 쪼개질 수 있는 단위로 생각한다. 예를 들어서 단어 'where'를 'wher'와 'ere'로 쪼갤 수 있다. 이렇게 쪼개진 단위를 n-gram이라고 한다. 패스트 텍스트는 이런 n-gram 단위로 임베딩한다. 이렇게 하면 단어를 더 잘 표현할 수 있다.  n-gram은 최소값과 최대값으로 범위를 설정할 수 이따.
```
# n = 3 ~ 6
apple = <ap + app + ppl + ppl + le> + <app + appl + pple + ple> + <appl + pple> + , ..., +<apple>
``
이런 방식은 Word2Vec에서는 없는 이점이 생긴다. FastText의 인공 신경망을 학습한 이후에는 데이터셋만 충분하면 학습되지 않은 단어도 내부 단어를 통해 임베딩된 벡터값을 얻을 수 있다. 이는 OOV 문제를 해결한다. 또한 이런 방식은 오타와 같은 매우 빈도수가 낮은 단어에도 유연하게 대처하기 때문에 좋은 임베딩 방식이 된다.

## Reference
[위키독스 - 딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/book/2155)