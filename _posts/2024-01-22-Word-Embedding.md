---
layout : post
title: Word Embedding
author: Hojoon_Kim
date: 2024-01-22 16:15:10 +0900
categories: [Develope, NLP]
tags: [DL, NLP, Word-Embedding]
pin: true
math: true
---
워드 임베딩이랑 단어를 벡터로 표현한믄 방법으로, 단어를 밀집 표현으로 변환하는 방법이다. 원-핫 인코딩은 단어를 표현하기 위해 벡터의 차원이 단어 집합의 크기만큼 필요했다. 원-핫 벡터는 단어의 유사도를 표현하지 못한다는 단점이 있다. 이를 위해 단어를 밀집 벡터로 표현하는 방법이 있다. 밀집 벡터는 원-핫 벡터와 달리 대부분의 값이 실수이다. 밀집 벡터는 단어의 의미를 잘 표현해준다. 이를 위해 사용하는 방법이 워드 임베딩이다. 워드 임베딩은 단어를 밀집 벡터로 만드는 것을 말한다. 워드 임베딩 방법으로는 LSA, Word2Vec, FastText, Glove 등이 있다.

## 밀집 표현 (Dense Representation)
희소 표현과 반대되는 표현으로, 벡터의 차원을 단어 집합의 크기로 상정하지 않는다. 그래서 대부분의 값이 실수이다. 이를 통해 단어의 의미를 잘 표현해준다.  
ex) [0.1, 0.2, 0.5, -0.9, 1.8]

## 워드투벡터(Word2Vec)
워드투벡터는 단어의 의미를 수치화 할 수 있는 방법이다. 예를 들자면  
한국 - 서울 + 도쿄 = 일본   
박찬호 - 야구 + 축구 = 호나우두 

와 같이 단어의 의미를 벡터화 할 수 있다.

### CBOW(Continuous Bag of Words)
 